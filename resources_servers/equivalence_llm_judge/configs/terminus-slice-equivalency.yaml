equivalence_llm_judge:
  resources_servers:
    equivalence_llm_judge:
      entrypoint: app.py
      judge_model_server:
        type: responses_api_models
        name: policy_model
      judge_responses_create_params:
        input: []
      judge_prompt_template: |-
        ===== System role =====
        You are a meticulous software engineering and terminal command grader.

        Task: Determine if the candidate's generated output keystroke(s) is functionally equivalent to the GOLD output keystroke(s). You can use the state analysis and explanation to help with your decision.
        If the "is_task_complete" for candidate and gold are different, mark as not equivalent.

        Show your reason why they are equivalent or not equivalent first and then provide the output.

        Output (at the end after double newlines):
        - If equivalent: [[A=B]] they are equivalent
        - If not equivalent: [[A!=B]] they are not equivalent

        ===== Example 1 (equivalent) =====
        GOLD:
        {{
          "state_analysis": "I have created a Python script to reproduce the issue. The next step is to run it and verify that it raises the expected `NameError`.",
          "explanation": "I will execute the script to observe the error triggered when calling `create_authorization_response`.",
          "commands": [
            {{
              "keystrokes": "python reproduce_issue.py\\n",
              "is_blocking": true,
              "timeout_sec": 3
            }}
          ],
          "is_task_complete": false
        }}

        CANDIDATE:
        {{
          "state_analysis": "I have created `reproduce_issue.py`, which is expected to trigger a `NameError` in oauthlib. The script has not been executed yet to confirm the failure or capture the stack trace.",
          "explanation": "I will run the script to reproduce the `NameError` so I can inspect the stack trace and identify where and why the bug occurs.",
          "commands": [
            {{
              "keystrokes": "python reproduce_issue.py\\n",
              "is_blocking": true,
              "timeout_sec": 5
            }}
          ],
          "is_task_complete": false
        }}

        The candidate's command is functionally equivalent to the GOLD command. The difference in timeout does not change the result or intent of the action in this context.

        [[A=B]] they are equivalent

        ===== Example 2 (not equivalent) =====
        GOLD:
        {{
          "state_analysis": "The `implicit_grant` assignments are correctly set to `ImplicitGrant` in both the `Server` class (line 133) and `MobileApplicationServer` class (line 43). The next step is to verify the `password_grant` assignments.",
          "explanation": "I will use `grep` to confirm that `password_grant` is correctly set to `ResourceOwnerPasswordCredentialsGrant`.",
          "commands": [
            {{
              "keystrokes": "grep -n \\"password_grant.*ResourceOwnerPasswordCredentialsGrant\\" /testbed/oauthlib/oauth2/rfc6749/endpoints/pre_configured.py\\n",
              "is_blocking": true,
              "timeout_sec": 5
            }}
          ],
          "is_task_complete": false
        }}

        CANDIDATE:
        {{
          "state_analysis": "The `implicit_grant` assignments now correctly use `ImplicitGrant` in both relevant server classes, matching the intended fix. To ensure overall correctness, additional related assignments and mappings should be verified, including `password_grant`, `BearerToken`, `response_types`, `grant_types`, and `default_response_type`.",
          "explanation": "I will search for key variables in the file to confirm that all targeted changes were applied correctly and that no related configuration issues remain.",
          "commands": [
            {{
              "keystrokes": "grep -n \\"password_grant\\" /testbed/oauthlib/oauth2/rfc6749/endpoints/pre_configured.py\\n",
              "is_blocking": true,
              "timeout_sec": 5
            }},
            {{
              "keystrokes": "grep -n \\"BearerToken(\\" /testbed/oauthlib/oauth2/rfc6749/endpoints/pre_configured.py\\n",
              "is_blocking": true,
              "timeout_sec": 5
            }},
            {{
              "keystrokes": "grep -n \\"default_response_type\\" /testbed/oauthlib/oauth2/rfc6749/endpoints/pre_configured.py\\n",
              "is_blocking": true,
              "timeout_sec": 5
            }},
            {{
              "keystrokes": "grep -n \\"response_types\\" /testbed/oauthlib/oauth2/rfc6749/endpoints/pre_configured.py\\n",
              "is_blocking": true,
              "timeout_sec": 5
            }},
            {{
              "keystrokes": "grep -n \\"grant_types\\" /testbed/oauthlib/oauth2/rfc6749/endpoints/pre_configured.py\\n",
              "is_blocking": true,
              "timeout_sec": 5
            }}
          ],
          "is_task_complete": false
        }}

        The candidate command is not functionally equivalent because it broadly searches for `password_grant` without enforcing that it is assigned to `ResourceOwnerPasswordCredentialsGrant`, whereas the GOLD command explicitly verifies the correct assignment in a single, precise check.

        [[A!=B]] they are not equivalent

        ===== Inputs =====
        GOLD:
        {expected_answer}

        CANDIDATE:
        {generated_answer}
      judge_endpoint_max_concurrency: null
      judge_system_message: null
      judge_equal_label: "[[A=B]]"
      judge_not_equal_label: "[[A!=B]]"

      # Optional regex to extract question from the last user message. The LAST
      # match is used. If capture groups exist, the first non-empty group is
      # returned; otherwise, the entire last match is used.
      # Example: "^Question:\\s*(.*)$"
      question_extract_regex: null

      # Optional regex to extract the generated response from the last assistant message.
      # The LAST match is used. If capture groups exist, the first non-empty
      # group is returned; otherwise, the entire last match is used.
      # Example: "^Answer:\\s*(.*)$"
      response_extract_regex: null

      # Swap check: Run second judge pass with swapped expected/generated to detect positional bias
      check_twice_swap: true
      # Reward when the second (swap) pass fails; default 0.0, can be -1.0
      reward_if_swap_fails: 0.0

      # ========================================================================
      # Per-Record Regex Features (OpenQA support)
      # ========================================================================
      # These features enable mixed datasets with different answer formats.
      # They only activate when template_metadata.output_regex is present.
      # Safe to enable by default - falls back to response_extract_regex when
      # no per-record regex is present.

      # [NEW] Enable per-record regex override from template_metadata.output_regex
      use_per_record_regex: false

      # --- The following features ONLY work when use_per_record_regex=true ---

      # [NEW] Skip regex extraction when expected_answer length exceeds this threshold.
      # When skipped, the full generation is shown to judge instead of extracting.
      # Only applies when per-record regex is present. Set to null to disable.
      extraction_length_threshold: 120

      # [NEW] If true, when first pass fails, retry with full generation (no regex) for partial credit.
      # Helps recover from regex extraction failures. Only activates when per-record regex exists.
      check_full_generation_on_fail: true

      # [NEW] Reward when full generation check succeeds after first pass fails.
      # Default 0.5 (partial credit). Set to 1.0 for full credit or 0.0 to ignore.
      reward_if_full_generation_succeeds: 0.5
      domain: agent
      verified: false
      description: single-step terminal based task
      value: Improve on terminal-style tasks
equivalence_llm_judge_simple_agent:
  responses_api_agents:
    simple_agent:
      entrypoint: app.py
      resources_server:
        type: resources_servers
        name: equivalence_llm_judge
      model_server:
        type: responses_api_models
        name: policy_model
      datasets:
      - name: example
        type: example
        license: "TBD"
        jsonl_fpath: /lustre/fsw/portfolios/llmservice/users/kbhardwaj/dev/Gym/data/nl2bash/example.jsonl
      - name: train
        type: train
        license: "TBD"
        jsonl_fpath: /lustre/fsw/portfolios/llmservice/users/kbhardwaj/dev/Gym/data/nl2bash/example.jsonl

