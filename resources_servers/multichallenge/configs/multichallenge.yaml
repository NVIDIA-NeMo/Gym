# ============================================================================
# MultiChallenge Environment Configuration
# ============================================================================
# This config defines the MultiChallenge environment and LLM judge setup.
#
# LLM Judge Configuration:
#   By default, the judge uses the same vLLM server as the policy model.
#   This is efficient as no additional GPU resources are needed. In practice,
#   we typically use a separate judge model server with a different model.
#
#   To use a separate judge endpoint, uncomment the judge_model section below
#   and change judge_model_server.name to "judge_model".
# ============================================================================

# ============================================================================
# MultiChallenge Resources Server
# ============================================================================
multichallenge:
  resources_servers:
    multichallenge:
      entrypoint: app.py

      # Judge model reference - uses the same vLLM server as the policy model.
      # This means generation and judging share the same model instance.
      # To use a separate judge, change 'name' to 'judge_model' and configure below.
      judge_model_server:
        type: responses_api_models
        name: policy_model

      # Parameters for judge requests
      judge_responses_create_params:
        input: []
        max_output_tokens: 8192
        temperature: 0.7
        top_p: 0.8

      # Aggregation mode for rubric scores: mean | min | max | all | any | weighted
      aggregation_mode: mean
      parallel_evaluation: true

      # Judge system message
      judge_system_message: >-
        You are a precise evaluator. Assess responses objectively based on the given criteria.
        Analyze the response carefully against the evaluation question.

      # Judge prompt template with placeholders
      judge_prompt_template: |-
        You are evaluating whether a model's response meets a specific criterion.

        CONVERSATION CONTEXT:
        {context}

        MODEL'S FINAL RESPONSE:
        {response}

        EVALUATION QUESTION:
        {question}

        EXPECTED ANSWER: {pass_criteria}

        Does the model's response satisfy the criterion described in the evaluation question?
        Think step by step, then respond with exactly [[YES]] or [[NO]] on the last line.

      # Verdict labels
      yes_label: "[[YES]]"
      no_label: "[[NO]]"

      domain: knowledge
      description: "MultiChallenge benchmark evaluation with LLM judge"

# ============================================================================
# MultiChallenge Agent Configuration
# ============================================================================
multichallenge_simple_agent:
  responses_api_agents:
    simple_agent:
      entrypoint: app.py
      resources_server:
        type: resources_servers
        name: multichallenge
      model_server:
        type: responses_api_models
        name: policy_model
      datasets:
        - name: multichallenge_example
          type: example
          license: "Apache 2.0"
          jsonl_fpath: resources_servers/multichallenge/data/example.jsonl
        - name: multichallenge_advanced
          type: train
          license: "TBD"
          jsonl_fpath: resources_servers/multichallenge/data/advanced.jsonl
        - name: multichallenge_vanilla
          type: train
          license: "TBD"
          jsonl_fpath: resources_servers/multichallenge/data/vanilla.jsonl

# ============================================================================
# Judge Model Server Configuration (Optional)
# ============================================================================
# By default, the judge uses 'policy_model' (configured above).
# This means the same vLLM server handles both generation and judging.
#
# If you want a SEPARATE judge model, uncomment ONE of the options below
# and change judge_model_server.name from 'policy_model' to 'judge_model'.
# ============================================================================

# ----------------------------------------------------------------------------
# OPTION: External OpenAI-compatible API for Judge
# ----------------------------------------------------------------------------
# Use this to connect to an external endpoint (NVIDIA NIM, OpenAI, etc.)
# Configure credentials in env.yaml or via command-line overrides.
#
# judge_model:
#   responses_api_models:
#     openai_model:
#       entrypoint: app.py
#       openai_base_url: ${judge_base_url}
#       openai_api_key: ${judge_api_key}
#       openai_model: ${judge_model_name}
